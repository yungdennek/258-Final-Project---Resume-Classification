# -*- coding: utf-8 -*-
"""ResumeClassificationBasicModels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ojLnMWw26x0mk88KiOizZTC0b-DXdPYj
"""

## Load the dataset
import pandas as pd

df = pd.read_csv("hf://datasets/AzharAli05/Resume-Screening-Dataset/dataset.csv")

df.head()

import re

## Clean the resumes and descriptions by removing filler words
words_to_omit = ["*", " for ", "(", ")", " a ", " the ", " in ", " and ", ",", "professional", " to ",
                 " with ", " motivated ", "[", "]", "{", "}", " of ", " by ", " using ", " that ", " at ",
                 " through ", "highly", " other ", "\n", "we're ", "looking", "seeking", "be ", " part ",
                 " an ", " are ", " you ", " us ", " is ", "here", "job description", " am ", " my ", " has "]

def cleanedText(text):
  cleanText = text.lower()
  for word in words_to_omit:
    cleanText = re.sub(re.escape(word), " ", cleanText)
  return cleanText

clean_resumes = []
clean_descriptions = []

resumes = df["Resume"].tolist()

for resume in resumes:
  clean_resumes.append(cleanedText(resume))

for description in df["Job_Description"].tolist():
  clean_descriptions.append(cleanedText(description))

df["Clean Resumes"] = clean_resumes
df["Clean Descriptions"] = clean_descriptions

## Turn the Decision column into a Binary result column
binary_values = []

for decision in df["Decision"].tolist():
  if decision == "reject":
    binary_values.append(0)
  else:
    binary_values.append(1)

df["Binary Result"] = binary_values

## Delete unnecessary column
if("Reason_for_decision" in df):
  del df["Reason_for_decision"]

df.head()

## Visualize each role in the dataset using a piechart
import matplotlib.pyplot as plt
import seaborn as sns

role_counts = df['Role'].value_counts()

plt.figure(figsize=(10, 10))
plt.pie(role_counts, labels=role_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Roles in the Dataset')
plt.axis('equal')
plt.show()

import numpy as np

## These are fundamental terms on a resume
key_terms = ["education:", "skills:", "summary:", "experience:",
             "achievements:", "certifications:", "references:"]

def resume_parser(index):
  test_resume = df["Clean Resumes"][index].lower()

  indices = []

  for term in key_terms:
    indices.append(test_resume.find(term))

  test_dict = dict(zip(indices, key_terms))
  test_dict = dict(sorted(test_dict.items()))

  resume_parsed = {}

  key_list = list(test_dict.keys())

  for i in range(len(key_list) - 1):
    key1, key2 = key_list[i], key_list[i + 1]
    resume_parsed[test_dict[key1]] = test_resume[key1:key2]

  return resume_parsed

## The most important packages for text analysis
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

st = SentenceTransformer("paraphrase-MiniLM-L6-v2")

## Compare the job description to each section of the resume and return the results
def similarities_to_description(index):
  description = df.loc[index]["Clean Descriptions"]
  resume = resume_parser(index)

  similarity_scores = []

  vectorizer = TfidfVectorizer()

  for term in key_terms:
    if(term not in resume):
      similarity_scores.append(0)
      continue
    corpus = [resume[term], description]
    tfidf_matrix = vectorizer.fit_transform(corpus)

    similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]

    similarity_scores.append(float(similarity_score))
  return similarity_scores

similarity_vectors = [similarities_to_description(i) for i in range(len(df))]
results = df["Binary Result"].tolist()

## Create, train, and visualize a model through tensorflow keras

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
import matplotlib.pyplot as plt

def create_model(X, y):
    X = np.array(X)
    y = np.array(y)

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    input_dim = X.shape[1]

    model = tf.keras.Sequential([tf.keras.layers.Input(shape=(input_dim,)), tf.keras.layers.Dense(64, activation="relu"), tf.keras.layers.Dense(32, activation="relu"), tf.keras.layers.Dense(1, activation="sigmoid")])

    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=16, verbose=0)

    loss, acc = model.evaluate(X_val, y_val, verbose=0)
    print("Validation accuracy:", acc)

    y_pred_binary = (model.predict(X) > 0.5).astype(int)
    cm = confusion_matrix(y, y_pred_binary)

    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
    disp.plot()
    plt.title("Confusion Matrix")
    plt.show()

    y_pred_prob = model.predict(X).ravel()
    fpr, tpr, thresholds = roc_curve(y, y_pred_prob)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

    return acc

create_model(similarity_vectors, results)

## Visualize the similarity of resume sections to job description, and compare hire vs reject
plt.figure(figsize=(12, 7))
sns.barplot(x='Resume Section', y='Average Similarity Score', hue='Binary Result', data=plot_df, palette={'Selected': 'skyblue', 'Rejected': 'salmon'})
plt.title('Average Similarity Scores by Resume Section and Hiring Decision')
plt.xlabel('Resume Section')
plt.ylabel('Average Similarity Score')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Hiring Decision')
plt.tight_layout()
plt.show()

def role_based_embeddings(role):
  embeddings = st.encode(df.loc[df["Role"].str.lower() == role.lower(), "Clean Resumes"].tolist())
  results = df.loc[df["Role"].str.lower() == role.lower(), "Binary Result"].tolist()
  return embeddings, results

def role_model(role):
  role = role.lower()
  embeddings, results = role_based_embeddings(role)
  if len(embeddings) > 1 and len(results) > 1:
      model = create_model(embeddings, results)
      return model
  else:
      print("Not enough data for role ", role, " to train a model.")
      return None

## Create and visualize the models for all roles
roles = [role.lower() for role in df["Role"].unique()]

role_accuracies = {}

for role in roles:
  if role in roles:
    embeddings, results = role_based_embeddings(role)
    if len(embeddings) > 0 and len(results) > 0:
      print("\nTraining model for role: ", role)
      acc = role_model(role)
      if acc is not None:
        role_accuracies[role] = acc
    else:
      print("Skipping role ", role, " due to insufficient data.")
  else:
    print("Skipping role ", role, " as it's not in the predefined roles list.")

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 8))
sns.barplot(x=list(role_accuracies.keys()), y=list(role_accuracies.values()), palette='viridis')
plt.title('Validation Accuracy of DNN for Each Role')
plt.xlabel('Role')
plt.ylabel('Validation Accuracy')
plt.xticks(rotation=90)
plt.ylim(0, 1) # Accuracy is between 0 and 1
plt.tight_layout()
plt.show()